{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to refine on the output of RDA_hovmoller by allowing us to create frequency diagrams for arbitrary time periods.\n",
    "#also, importantly, now shows Hovmoller plot of intensity for idfferent time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Access NetCDF files (currently hosted in Ferret directory)\n",
    "RDA_path_1 = \"/Users/Siwen/Desktop/ferret/bin/meiyu_clean.nc\"\n",
    "RDA_path_2 = \"/Users/Siwen/Desktop/ferret/bin/meiyu_2_clean.nc\"\n",
    "RDA_1 = nc.Dataset(RDA_path_1, 'r')\n",
    "RDA_2 = nc.Dataset(RDA_path_2, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data from NetCDF files to notebook\n",
    "lat_1_all =  RDA_1.variables['lat_115'][:]\n",
    "lat_2_all =  RDA_2.variables['lat_115'][:]\n",
    "intensity_1_all = RDA_1.variables['intensity'][:] \n",
    "intensity_2_all = RDA_2.variables['intensity'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign a calendar date to each time point\n",
    "startday = datetime.datetime(1951,1,1)\n",
    "date_list = np.array([datetime.timedelta(days=x) + startday for x in range(0, 20819)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define empty variables reshaped with axis 0 as day of year and axis 1 as year.\n",
    "\n",
    "#time periods of interest - period1 (p1) and period2 (p2)\n",
    "#adjustment: mymap (below) now also restricts by years of interest.\n",
    "p1_yr_b = [1951,1980]\n",
    "p1_yr_e = [1979,1993]\n",
    "p2_yr_b = [1980,1994]\n",
    "p2_yr_e = [2007,2007]\n",
    "\n",
    "#variable storage for period 1 and period 2 - initialized as e\n",
    "lat_1_p1 = []\n",
    "lat_2_p1 = []\n",
    "intensity_1_p1 = []\n",
    "intensity_2_p1 = []\n",
    "\n",
    "lat_1_p2 = []\n",
    "lat_2_p2 = []\n",
    "intensity_1_p2 = []\n",
    "intensity_2_p2 = []\n",
    "\n",
    "## TIME PERIOD 1 - arrange data by day of year and then year\n",
    "for p1_b, p1_e in zip(p1_yr_b, p1_yr_e):\n",
    "\n",
    "    lat_1 = np.zeros([365, p1_e-p1_b+1])\n",
    "    lat_2 = np.zeros([365, p1_e-p1_b+1])\n",
    "    int_1 = np.zeros([365, p1_e-p1_b+1])\n",
    "    int_2 = np.zeros([365, p1_e-p1_b+1])\n",
    "        \n",
    "    for i, date in enumerate(date_list[0:365]): #go through each day of the year\n",
    "    \n",
    "        dd = date.day\n",
    "        mm = date.month\n",
    "        mymap = map(lambda date: (date.day == dd) & (date.month == mm) & (date.year >= p1_b) & (date.year <= p1_e), date_list)\n",
    "        extract = np.array(list(mymap))\n",
    "    \n",
    "        lat_1[i,:] = lat_1_all[extract]\n",
    "        lat_2[i,:] = lat_2_all[extract]\n",
    "        int_1[i,:] = intensity_1_all[extract]\n",
    "        int_2[i,:] = intensity_2_all[extract]\n",
    "                    \n",
    "    lat_1_p1.append(lat_1)\n",
    "    lat_2_p1.append(lat_2)\n",
    "    intensity_1_p1.append(int_1)\n",
    "    intensity_2_p1.append(int_2)\n",
    "    \n",
    "## TIME PERIOD 2    \n",
    "for p2_b, p2_e in zip(p2_yr_b, p2_yr_e):\n",
    "\n",
    "    lat_1 = np.zeros([365, p2_e-p2_b+1])\n",
    "    lat_2 = np.zeros([365, p2_e-p2_b+1])\n",
    "    int_1 = np.zeros([365, p2_e-p2_b+1])\n",
    "    int_2 = np.zeros([365, p2_e-p2_b+1])\n",
    "        \n",
    "    for i, date in enumerate(date_list[0:365]): #go through each day of the year\n",
    "    \n",
    "        dd = date.day\n",
    "        mm = date.month\n",
    "        mymap = map(lambda date: (date.day == dd) & (date.month == mm) & (date.year >= p2_b) & (date.year <= p2_e), date_list)\n",
    "        extract = np.array(list(mymap))\n",
    "    \n",
    "        lat_1[i,:] = lat_1_all[extract]\n",
    "        lat_2[i,:] = lat_2_all[extract]\n",
    "        int_1[i,:] = intensity_1_all[extract]\n",
    "        int_2[i,:] = intensity_2_all[extract]\n",
    "                    \n",
    "    lat_1_p2.append(lat_1)\n",
    "    lat_2_p2.append(lat_2)\n",
    "    intensity_1_p2.append(int_1)\n",
    "    intensity_2_p2.append(int_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 29)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_1_p1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert latitude information into binning by latitude\n",
    "#BINS: <20, 20-40 by 1 degree bin, and 40+\n",
    "lat_1_index = np.round(lat_1-19.5)\n",
    "lat_1_index[lat_1_index < 0] = 0\n",
    "lat_1_index[lat_1_index > 21] = 21\n",
    "\n",
    "lat_2_index = np.round(lat_2-19.5)\n",
    "lat_2_index[lat_2_index < 0] = 0\n",
    "lat_2_index[lat_2_index > 21] = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert indices into Hovmoller plot\n",
    "lat_1_hov = np.zeros([365,22])\n",
    "lat_2_hov = np.zeros([365,22])\n",
    "\n",
    "#below, we divide by 57 (the number of years in our data set) to switch units from raw counts to frequency.\n",
    "\n",
    "#for primary fronts:\n",
    "for dd, col in enumerate(lat_1_index):\n",
    "    for ix in range(22):\n",
    "        lat_1_hov[dd,ix] = np.count_nonzero(col == ix)/57\n",
    "        \n",
    "#for secondary fronts:\n",
    "for dd, col in enumerate(lat_2_index):\n",
    "    for ix in range(22):\n",
    "        lat_2_hov[dd,ix] = np.count_nonzero(col == ix)/57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine into single variable\n",
    "lat_hov = lat_1_hov + lat_2_hov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#handy smoothing function which allows for different smoothing in time direction and latitude direction, and also\n",
    "#wraps time axis\n",
    "def smooth(P,dayrange,latrange):\n",
    "    Psmooth=np.zeros(P.shape)\n",
    "    \n",
    "    ll=int((dayrange-1)/2)\n",
    "    yy=int((latrange-1)/2)\n",
    "    \n",
    "    days = P.shape[0]\n",
    "    lats = P.shape[1]\n",
    "    \n",
    "    for d in range(days):\n",
    "        for j in range(lats):\n",
    "            \n",
    "            Psample = P.take(range(d-ll,d+ll+1), mode='wrap', axis=0)\n",
    "            Ps = Psample[:,max(0,j-yy):min(days,j+yy+1)]\n",
    "            Psmooth[d,j] = np.mean(np.mean(Ps))\n",
    "    \n",
    "    return Psmooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#smoothed occupancy plots for frontal events\n",
    "lat_1_hov_smth = smooth(lat_1_hov,15,5)\n",
    "lat_2_hov_smth = smooth(lat_2_hov,15,5)\n",
    "lat_hov_smth = smooth(lat_hov,15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SAVE AS NETCDF FILE\n",
    "savefile = \"RDA_hov_\"\n",
    "ferretpath = \"/Users/Siwen/Desktop/Ferret/bin/\"\n",
    "\n",
    "#netCDF output file formatting formatting\n",
    "hovname = ferretpath + savefile + \".nc\"\n",
    "\n",
    "##the following is only to make sure that we don't exceed the bounds of the number of rows of actual data.\n",
    "if os.path.isfile(hovname):\n",
    "    os.remove(hovname)\n",
    "\n",
    "hovout = nc.Dataset(hovname, \"w\")\n",
    "\n",
    "#create dimensions\n",
    "days_dim = hovout.createDimension('time', None)\n",
    "lat_dim = hovout.createDimension('lat', 22)\n",
    "\n",
    "dim_tuple = (\"time\",\"lat\")\n",
    "\n",
    "#create variables inside netCDF file\n",
    "lat_1_out = hovout.createVariable(\"lat_1_hov\", 'f8', dim_tuple)\n",
    "lat_2_out = hovout.createVariable(\"lat_2_hov\", 'f8', dim_tuple)\n",
    "lat_out = hovout.createVariable(\"lat_hov\", 'f8', dim_tuple)\n",
    "\n",
    "lat_1_smth_out = hovout.createVariable(\"lat_1_hov_smth\", 'f8', dim_tuple)\n",
    "lat_2_smth_out = hovout.createVariable(\"lat_2_hov_smth\", 'f8', dim_tuple)\n",
    "lat_smth_out = hovout.createVariable(\"lat_hov_smth\", 'f8', dim_tuple)\n",
    "\n",
    "lat_1_out[:] = lat_1_hov\n",
    "lat_2_out[:] = lat_2_hov\n",
    "lat_out[:] = lat_hov\n",
    "\n",
    "lat_1_smth_out[:] = lat_1_hov_smth\n",
    "lat_2_smth_out[:] = lat_2_hov_smth\n",
    "lat_smth_out[:] = lat_hov_smth\n",
    "\n",
    "hovout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
